{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f6a5090"
      },
      "source": [
        "# Simple Impementation of E(n) Equivariant Graph Neural Networks\n",
        "\n",
        "Original paper https://arxiv.org/pdf/2102.09844.pdf by Victor Garcia Satorras, Emiel Hoogeboom, Max Welling"
      ],
      "id": "6f6a5090"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bU4ixrOJCg1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "4bU4ixrOJCg1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cb08a10"
      },
      "source": [
        "# Load QM9 Dataset"
      ],
      "id": "8cb08a10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae30de9d",
        "outputId": "fa17dbf7-2961-4160-ecba-0f9ac2e89c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'simple-equivariant-gnn'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 87 (delta 37), reused 31 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n",
            "/content/simple-equivariant-gnn\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/senya-ashukha/simple-equivariant-gnn.git\n",
        "%cd simple-equivariant-gnn"
      ],
      "id": "ae30de9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "859f981c",
        "outputId": "781d8af5-c184-45fd-89a0-035c19e7c45f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys([0, 1, 6, 7, 8, 9])\n",
            "dict_keys([0, 1, 6, 7, 8, 9])\n",
            "dict_keys([0, 1, 6, 7, 8, 9])\n"
          ]
        }
      ],
      "source": [
        "# QM9 is a dataset for Molecular Property Predictions http://quantum-machine.org/datasets/\n",
        "# We will predict Highest occupied molecular orbital energy \n",
        "# https://en.wikipedia.org/wiki/HOMO_and_LUMO\n",
        "# We use data loaders from the official repo\n",
        "\n",
        "from qm9.data_utils import get_data, BatchGraph\n",
        "train_loader, val_loader, test_loader, charge_scale = get_data(num_workers=1)"
      ],
      "id": "859f981c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05e20004"
      },
      "source": [
        "# Graph Representation"
      ],
      "id": "05e20004"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0acbcc0",
        "outputId": "ebedd03b-bdbf-4c22-fd5f-a397bcbc68a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "In the batch: num_graphs 96 num_nodes 1765\n",
              "> .h \t\t a tensor of nodes representations \t\tshape 1765 x 15\n",
              "> .x \t\t a tensor of nodes positions  \t\t\tshape 1765 x 3\n",
              "> .edges \t a tensor of edges, a fully connected graph \tshape 31564 x 2\n",
              "> .batch  \t a tensor of graph_ids for each node \t\ttensor([ 0,  0,  0,  ..., 95, 95, 95])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
        "batch"
      ],
      "id": "d0acbcc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "784c0726"
      },
      "source": [
        "# Define Equivariant Graph Convs  & GNN"
      ],
      "id": "784c0726"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76e5e05f"
      },
      "outputs": [],
      "source": [
        "def index_sum(agg_size, source, idx, cuda):\n",
        "    \"\"\"\n",
        "        source is N x hid_dim [float]\n",
        "        idx    is N           [int]\n",
        "        \n",
        "        Sums the rows source[.] with the same idx[.];\n",
        "    \"\"\"\n",
        "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
        "    tmp = tmp.cuda() if cuda else tmp\n",
        "    res = torch.index_add(tmp, 0, idx, source)\n",
        "    return res"
      ],
      "id": "76e5e05f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d5d55db"
      },
      "outputs": [],
      "source": [
        "class ConvEGNN(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        self.cuda = cuda\n",
        "        \n",
        "        # computes messages based on hidden representations -> [0, 1]\n",
        "        self.f_e = nn.Sequential(nn.Linear(2 * in_dim + 1, hid_dim), nn.SiLU(),\n",
        "                                 nn.Linear(hid_dim, hid_dim), nn.SiLU())\n",
        "        \n",
        "        # preducts \"soft\" edges based on messages \n",
        "        self.f_inf = nn.Sequential(nn.Linear(hid_dim, 1), nn.Sigmoid())\n",
        "        \n",
        "        # updates hidden representations -> [0, 1]\n",
        "        self.f_h = nn.Sequential(nn.Linear(in_dim + hid_dim, hid_dim), nn.SiLU(), \n",
        "                                 nn.Linear(hid_dim, hid_dim))\n",
        "\n",
        "        \n",
        "    def forward(self, b):\n",
        "        # compute distances for all edges\n",
        "        edge_st, edge_end = b.edges[:,0], b.edges[:,1]\n",
        "        dists = torch.sum((b.x[edge_end] - b.x[edge_st]) ** 2, dim=1).reshape(-1, 1)\n",
        "        \n",
        "        # compute messages\n",
        "        tmp = torch.hstack([b.h[edge_st], b.h[edge_end], dists])\n",
        "        m_ij = self.f_e(tmp)\n",
        "        \n",
        "        # predict edges\n",
        "        e_ij = self.f_inf(m_ij)\n",
        "        \n",
        "        # average e_ij-weighted messages  \n",
        "        # m_i is num_nodes x hid_dim\n",
        "        m_i = index_sum(b.h.shape[0], e_ij*m_ij, b.edges[:,0], self.cuda)\n",
        "        \n",
        "        # update hidden representations\n",
        "        b.h += self.f_h(torch.hstack([b.h, m_i]))\n",
        "\n",
        "        return b"
      ],
      "id": "4d5d55db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10aad7c4"
      },
      "outputs": [],
      "source": [
        "class NetEGNN(nn.Module):\n",
        "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        \n",
        "        self.emb = nn.Linear(in_dim, hid_dim)\n",
        "\n",
        "        # Make gnn of n_layers\n",
        "        self.gnn = [ConvEGNN(hid_dim, hid_dim, cuda=cuda) for l in range(n_layers)]\n",
        "        self.gnn = nn.Sequential(*self.gnn)\n",
        "        \n",
        "        self.pre_mlp = nn.Sequential(\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim))\n",
        "        \n",
        "        self.post_mlp = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, out_dim))\n",
        "\n",
        "        if cuda: self.cuda()\n",
        "        self.cuda = cuda\n",
        "    \n",
        "    def forward(self, b):\n",
        "        b.h = self.emb(b.h)\n",
        "        \n",
        "        b = self.gnn(b)\n",
        "        h_nodes = self.pre_mlp(b.h)\n",
        "        \n",
        "        # h_graph is num_graphs x hid_dim\n",
        "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda) \n",
        "        \n",
        "        out = self.post_mlp(h_graph)\n",
        "        return out"
      ],
      "id": "10aad7c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7f4cef6"
      },
      "outputs": [],
      "source": [
        "epochs = 200\n",
        "cuda = True\n",
        "\n",
        "model = NetEGNN(n_layers=7, cuda=cuda)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16) # lr=5e-4\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)"
      ],
      "id": "b7f4cef6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e5d6b1c"
      },
      "source": [
        "# Training"
      ],
      "id": "4e5d6b1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "de3613c9",
        "outputId": "fd9b7c47-9a80-4294-a19e-f0e488fa6ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> start training\n",
            "> epoch 000: train 331.462 val 307.934 test 310.421 (85.8 sec)\n",
            "> epoch 001: train 228.864 val 233.028 test 228.634 (82.7 sec)\n",
            "> epoch 002: train 185.693 val 166.203 test 163.385 (82.9 sec)\n",
            "> epoch 003: train 163.003 val 137.879 test 136.068 (82.6 sec)\n",
            "> epoch 004: train 145.501 val 143.238 test 145.194 (82.7 sec)\n",
            "> epoch 005: train 132.975 val 120.555 test 120.356 (82.7 sec)\n",
            "> epoch 006: train 123.954 val 114.002 test 112.166 (82.5 sec)\n",
            "> epoch 007: train 117.180 val 106.519 test 106.249 (83.0 sec)\n",
            "> epoch 008: train 109.622 val 104.664 test 103.775 (82.4 sec)\n",
            "> epoch 009: train 105.312 val 98.981 test 100.688 (82.5 sec)\n",
            "> epoch 010: train 100.985 val 86.974 test 87.005 (82.4 sec)\n",
            "> epoch 011: train 97.742 val 99.523 test 98.275 (82.7 sec)\n",
            "> epoch 012: train 93.679 val 87.961 test 86.577 (81.9 sec)\n",
            "> epoch 013: train 91.089 val 82.321 test 81.607 (82.7 sec)\n",
            "> epoch 014: train 89.260 val 78.265 test 79.155 (82.3 sec)\n",
            "> epoch 015: train 86.124 val 93.564 test 94.063 (81.9 sec)\n",
            "> epoch 016: train 83.674 val 89.588 test 88.301 (82.3 sec)\n",
            "> epoch 017: train 82.810 val 77.788 test 77.848 (82.2 sec)\n",
            "> epoch 018: train 80.708 val 82.068 test 82.041 (82.7 sec)\n",
            "> epoch 019: train 78.289 val 70.671 test 70.918 (82.6 sec)\n",
            "> epoch 020: train 77.440 val 80.513 test 80.675 (82.0 sec)\n",
            "> epoch 021: train 76.575 val 79.602 test 79.280 (82.3 sec)\n",
            "> epoch 022: train 75.492 val 70.593 test 71.169 (82.0 sec)\n",
            "> epoch 023: train 73.501 val 78.103 test 78.806 (82.1 sec)\n",
            "> epoch 024: train 73.199 val 71.639 test 71.261 (81.7 sec)\n",
            "> epoch 025: train 72.054 val 66.540 test 66.636 (82.2 sec)\n",
            "> epoch 026: train 70.941 val 73.602 test 73.720 (82.0 sec)\n",
            "> epoch 027: train 70.326 val 64.465 test 64.461 (82.2 sec)\n",
            "> epoch 028: train 68.806 val 62.894 test 63.070 (82.3 sec)\n",
            "> epoch 029: train 68.233 val 67.236 test 67.712 (82.1 sec)\n",
            "> epoch 030: train 66.904 val 62.353 test 62.876 (82.1 sec)\n",
            "> epoch 031: train 67.474 val 63.163 test 63.934 (82.1 sec)\n",
            "> epoch 032: train 66.082 val 58.909 test 59.801 (82.6 sec)\n",
            "> epoch 033: train 64.931 val 66.753 test 67.314 (81.9 sec)\n",
            "> epoch 034: train 64.819 val 61.456 test 61.878 (82.0 sec)\n",
            "> epoch 035: train 64.011 val 57.051 test 57.236 (82.1 sec)\n",
            "> epoch 036: train 62.929 val 57.962 test 58.967 (81.8 sec)\n",
            "> epoch 037: train 62.673 val 54.327 test 54.877 (81.8 sec)\n",
            "> epoch 038: train 62.128 val 57.845 test 58.280 (81.6 sec)\n",
            "> epoch 039: train 61.502 val 58.576 test 58.653 (81.8 sec)\n",
            "> epoch 040: train 60.716 val 57.629 test 57.823 (82.1 sec)\n",
            "> epoch 041: train 60.293 val 57.463 test 58.056 (81.9 sec)\n",
            "> epoch 042: train 60.146 val 58.585 test 58.866 (82.3 sec)\n",
            "> epoch 043: train 59.349 val 56.384 test 56.300 (81.9 sec)\n",
            "> epoch 044: train 58.558 val 56.263 test 56.833 (82.5 sec)\n",
            "> epoch 045: train 57.933 val 57.180 test 57.508 (81.8 sec)\n",
            "> epoch 046: train 57.633 val 57.719 test 57.293 (82.3 sec)\n",
            "> epoch 047: train 56.408 val 55.747 test 56.689 (81.9 sec)\n",
            "> epoch 048: train 56.359 val 57.242 test 57.120 (82.3 sec)\n",
            "> epoch 049: train 55.852 val 51.916 test 51.544 (82.0 sec)\n",
            "> epoch 050: train 55.209 val 51.818 test 52.872 (81.9 sec)\n",
            "> epoch 051: train 54.799 val 51.307 test 51.991 (82.4 sec)\n",
            "> epoch 052: train 54.506 val 53.474 test 53.452 (81.9 sec)\n",
            "> epoch 053: train 55.330 val 51.216 test 51.417 (82.0 sec)\n",
            "> epoch 054: train 54.026 val 53.131 test 53.679 (81.9 sec)\n",
            "> epoch 055: train 52.720 val 56.324 test 57.178 (82.0 sec)\n",
            "> epoch 056: train 52.629 val 50.629 test 51.242 (81.6 sec)\n",
            "> epoch 057: train 51.791 val 52.850 test 53.798 (82.0 sec)\n",
            "> epoch 058: train 51.792 val 50.940 test 51.800 (81.9 sec)\n",
            "> epoch 059: train 51.345 val 48.480 test 49.193 (81.5 sec)\n",
            "> epoch 060: train 50.875 val 51.825 test 52.035 (81.8 sec)\n",
            "> epoch 061: train 50.070 val 49.614 test 50.065 (81.4 sec)\n",
            "> epoch 062: train 49.849 val 51.668 test 52.146 (82.0 sec)\n",
            "> epoch 063: train 50.750 val 51.376 test 52.226 (82.8 sec)\n",
            "> epoch 064: train 53.348 val 58.782 test 58.961 (83.4 sec)\n",
            "> epoch 065: train 50.023 val 48.565 test 49.255 (83.8 sec)\n",
            "> epoch 066: train 48.195 val 47.599 test 48.261 (83.3 sec)\n",
            "> epoch 067: train 47.105 val 48.161 test 48.662 (83.5 sec)\n",
            "> epoch 068: train 47.324 val 48.975 test 49.731 (83.3 sec)\n",
            "> epoch 069: train 47.274 val 47.202 test 47.514 (83.5 sec)\n",
            "> epoch 070: train 46.457 val 49.029 test 49.246 (83.3 sec)\n",
            "> epoch 071: train 46.318 val 48.571 test 49.013 (83.1 sec)\n",
            "> epoch 072: train 45.839 val 48.207 test 48.763 (83.4 sec)\n",
            "> epoch 073: train 45.543 val 46.066 test 46.340 (82.8 sec)\n",
            "> epoch 074: train 45.218 val 45.923 test 46.307 (83.6 sec)\n",
            "> epoch 075: train 44.726 val 47.888 test 48.507 (82.8 sec)\n",
            "> epoch 076: train 44.242 val 45.968 test 46.234 (83.0 sec)\n",
            "> epoch 077: train 44.258 val 46.553 test 47.250 (82.8 sec)\n",
            "> epoch 078: train 48.667 val 46.638 test 47.405 (83.2 sec)\n",
            "> epoch 079: train 42.966 val 44.933 test 45.001 (83.9 sec)\n",
            "> epoch 080: train 42.337 val 49.939 test 49.996 (83.7 sec)\n",
            "> epoch 081: train 42.326 val 43.228 test 44.065 (83.6 sec)\n",
            "> epoch 082: train 41.967 val 45.385 test 46.403 (83.2 sec)\n",
            "> epoch 083: train 41.859 val 46.051 test 46.678 (83.7 sec)\n",
            "> epoch 084: train 41.281 val 44.412 test 45.644 (83.6 sec)\n",
            "> epoch 085: train 41.276 val 47.590 test 47.765 (83.5 sec)\n",
            "> epoch 086: train 40.992 val 46.109 test 46.715 (83.3 sec)\n",
            "> epoch 087: "
          ]
        }
      ],
      "source": [
        "print('> start training')\n",
        "\n",
        "tr_ys = train_loader.dataset.data['homo']\n",
        "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
        "\n",
        "if cuda:\n",
        "    me = me.cuda()\n",
        "    mad = mad.cuda()\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True) \n",
        "    start = time.time()\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_val_loss = []\n",
        "    batch_test_loss = []\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = BatchGraph(batch, cuda, charge_scale)\n",
        "        \n",
        "        out = model(batch).reshape(-1)\n",
        "        loss =  F.l1_loss(out, (batch.y-me)/mad)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            loss =  F.l1_loss(out*mad+me, batch.y)\n",
        "\n",
        "        batch_train_loss += [float(loss.data.cpu().numpy())]  \n",
        "        \n",
        "    train_loss += [np.mean(batch_train_loss)/0.001]\n",
        "    \n",
        "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch in val_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
        "            batch_val_loss += [np.mean(loss)]\n",
        "            \n",
        "        val_loss += [np.mean(batch_val_loss)/0.001]\n",
        "        \n",
        "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
        "        \n",
        "        for batch in test_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
        "            batch_test_loss += [np.mean(loss)]\n",
        "\n",
        "        test_loss += [np.mean(batch_test_loss)/0.001]\n",
        "        \n",
        "    end = time.time()\n",
        "\n",
        "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
        "    lr_scheduler.step()"
      ],
      "id": "de3613c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFc-_flN9_I_"
      },
      "outputs": [],
      "source": [
        "print('Best result: ', test_loss[-1])\n",
        "plt.plot(test_loss[::10])\n",
        "plt.ylabel('Cost')\n",
        "plt.xlabel('Iterations (by 10)')   \n",
        "plt.show()"
      ],
      "id": "QFc-_flN9_I_"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}